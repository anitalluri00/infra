apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: mis-slo-rules
  namespace: observability
  labels:
    app: mis
    role: slo-rules
spec:
  groups:
    - name: mis-api-slo
      rules:
        - alert: MISAPIHigh5xxRate
          expr: |
            (
              sum(rate(nginx_ingress_controller_requests{namespace="mis-prod",service="backend-active",status=~"5.."}[5m]))
              /
              clamp_min(sum(rate(nginx_ingress_controller_requests{namespace="mis-prod",service="backend-active"}[5m])), 1)
            ) > 0.01
          for: 10m
          labels:
            severity: critical
            slo: availability
          annotations:
            summary: "MIS API 5xx error ratio > 1%"
            description: "Backend API is breaching error-rate SLO for 10 minutes."

        - alert: MISAPIP95LatencyHigh
          expr: |
            histogram_quantile(
              0.95,
              sum by (le) (
                rate(nginx_ingress_controller_request_duration_seconds_bucket{namespace="mis-prod",service="backend-active"}[5m])
              )
            ) > 0.5
          for: 15m
          labels:
            severity: warning
            slo: latency
          annotations:
            summary: "MIS API p95 latency > 500ms"
            description: "API latency SLO at risk for sustained traffic."

        - alert: MISBackendLowAvailability
          expr: |
            avg_over_time(
              (
                1 - (
                  sum(rate(nginx_ingress_controller_requests{namespace="mis-prod",service="backend-active",status=~"5.."}[5m]))
                  /
                  clamp_min(sum(rate(nginx_ingress_controller_requests{namespace="mis-prod",service="backend-active"}[5m])), 1)
                )
              )[1h:5m]
            ) < 0.999
          for: 30m
          labels:
            severity: critical
            slo: availability
          annotations:
            summary: "MIS availability below 99.9% objective"
            description: "Rolling availability is below target objective."

    - name: mis-worker-slo
      rules:
        - alert: MISCeleryHighCPU
          expr: |
            sum(rate(container_cpu_usage_seconds_total{namespace="mis-prod",pod=~"celery-worker-.*",container="celery"}[5m]))
            /
            clamp_min(sum(kube_pod_container_resource_limits{namespace="mis-prod",pod=~"celery-worker-.*",container="celery",resource="cpu"}), 1)
            > 0.8
          for: 15m
          labels:
            severity: warning
            slo: worker-capacity
          annotations:
            summary: "Celery workers sustained CPU > 80%"
            description: "Scale workers or investigate queue load profile."
